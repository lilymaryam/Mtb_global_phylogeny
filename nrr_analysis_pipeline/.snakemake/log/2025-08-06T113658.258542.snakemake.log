Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 5
Rules claiming more threads will be scaled down.
Job stats:
job         count    min threads    max threads
--------  -------  -------------  -------------
all             1              1              1
make_vis        1              1              1
total           2              1              1

Select jobs to execute...

[Wed Aug  6 11:36:58 2025]
rule make_vis:
    input: results/true1.sim1, results/true1.sim5, results/true1.sim10, results/true3.sim1, results/true3.sim5, results/true3.sim10, results/true5.sim1, results/true5.sim5, results/true5.sim10, results/true8.sim1, results/true8.sim5, results/true8.sim10, results/true10.sim1, results/true10.sim5, results/true10.sim10
    output: results/nrr_visual.svg
    jobid: 1
    resources: tmpdir=/tmp

[Wed Aug  6 11:37:00 2025]
Finished job 1.
1 of 2 steps (50%) done
Select jobs to execute...

[Wed Aug  6 11:37:00 2025]
localrule all:
    input: results/nrr_visual.svg
    jobid: 0
    resources: tmpdir=/tmp

[Wed Aug  6 11:37:00 2025]
Finished job 0.
2 of 2 steps (100%) done
Complete log: /home/lily/scripts/Mtb_global_phylogeny/nrr_analysis_pipeline/.snakemake/log/2025-08-06T113658.258542.snakemake.log
